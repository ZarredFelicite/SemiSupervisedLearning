{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>FixMatch</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from randaugment import RandAugmentMC\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# For rotation\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "#Logging\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = os.getcwd()+\"/CNN-VAE/data\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All images are 3x96x96\n",
    "image_size = 96\n",
    "#Example batch size\n",
    "batch_size = 24\n",
    "# Number of classes for classification\n",
    "out_classes = 10\n",
    "\n",
    "#temp = 0.5 Pseudo label temperature\n",
    "mu = 20 #Coefficient of unlabelled batch size\n",
    "threshold = 0.85 #Pseudo label threshold\n",
    "lambda_u = 8  #Coefficient of unlabelled loss\n",
    "\n",
    "start_from_checkpoint = False\n",
    "save_dir = 'Models'\n",
    "model_name = 'FixMatch'\n",
    "    \n",
    "# Hardware acceleration\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(image_size, padding=12, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomAffine(10, translate=(0.1,0.1)),\n",
    "    #RandAugmentMC(n=1, m=3),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "stl10_mean = (0.485, 0.456, 0.406)\n",
    "stl10_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "class TransformFix(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.weak = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=96,\n",
    "                                  padding=int(96*0.125),\n",
    "                                  padding_mode='reflect')])\n",
    "        self.strong = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=96,\n",
    "                                  padding=int(96*0.125),\n",
    "                                  padding_mode='reflect'),\n",
    "            RandAugmentMC(n=20, m=10)])\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=mean, std=std)\n",
    "            ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        weak = self.weak(x)\n",
    "        strong = self.strong(x)\n",
    "        return self.normalize(weak), self.normalize(strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train: 500 Val: 4500 Test: 8000\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=True)\n",
    "\n",
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=TransformFix(mean=stl10_mean, std=stl10_std), download=True)\n",
    "\n",
    "class_labels = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]\n",
    "print(\"Train: {} Val: {} Test: {}\".format(len(train_set),len(val_set),len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, drop_last=True, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=int(batch_size*mu), pin_memory=True, drop_last=True)#, persistent_workers=True)#, num_workers=4, persistent_workers=True)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, drop_last=True, num_workers=num_workers, pin_memory=True, persistent_workers=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, drop_last=True, num_workers=num_workers, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeled_iter = iter(train_loader)\n",
    "# unlabeled_iter = iter(unlabelled_loader)\n",
    "\n",
    "# #This Function will allow us to scale an images pixel values to a value between 0 and 1\n",
    "# def normalize_img(img):\n",
    "#     mins = img.min(0, keepdims = True).min(1, keepdims = True)\n",
    "#     maxs = img.max(0, keepdims = True).max(1, keepdims = True)\n",
    "#     return (img - mins)/(maxs - mins)\n",
    "\n",
    "# #Visualise strongly augmented unlabeled images\n",
    "# plt.figure(figsize = (15,10))\n",
    "# (image_batch_w, image_batch_s),_ = unlabeled_iter.next()\n",
    "# for i in range(4):    \n",
    "#     img_s = np.moveaxis(image_batch_s[i].numpy(),0,2)\n",
    "#     img_w = np.moveaxis(image_batch_w[i].numpy(),0,2)\n",
    "#     plt.subplot(2,4,i+1)\n",
    "#     plt.imshow(img_w)\n",
    "#     plt.subplot(2,4,i+5)\n",
    "#     plt.imshow(img_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch\n"
     ]
    }
   ],
   "source": [
    "#Create Save Path from save_dir and model_name, we will save and load our checkpoint here\n",
    "Save_Path = os.path.join(save_dir, model_name + \".pt\")\n",
    "\n",
    "#Create the save directory if it does not exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "#Load Checkpoint\n",
    "if start_from_checkpoint:\n",
    "    #Check if checkpoint exists\n",
    "    if os.path.isfile(Save_Path):\n",
    "        #load Checkpoint\n",
    "        check_point = torch.load(Save_Path)\n",
    "        #Checkpoint is saved as a python dictionary\n",
    "        #https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        #here we unpack the dictionary to get our previous training states\n",
    "        net.load_state_dict(check_point['model_state_dict'])\n",
    "        optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "        start_epoch = check_point['epoch']\n",
    "        best_valid_acc = check_point['valid_acc']\n",
    "        print(\"Checkpoint loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        #Raise Error if it does not exist\n",
    "        raise ValueError(\"Checkpoint Does not exist\")\n",
    "else:\n",
    "    #If checkpoint does exist and Start_From_Checkpoint = False\n",
    "    #Raise an error to prevent accidental overwriting\n",
    "    if os.path.isfile(Save_Path):\n",
    "        print(\"Warning Checkpoint exists\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device, labeled_trainloader, unlabeled_trainloader, test_loader, optimizer, mu, threshold, lambda_u, epoch):\n",
    "    #Train mode\n",
    "    net.train()\n",
    "    try:\n",
    "        inputs_x, targets_x = labeled_iter.next()\n",
    "    except:\n",
    "        labeled_iter = iter(labeled_trainloader)\n",
    "        inputs_x, targets_x = labeled_iter.next()\n",
    "    try:\n",
    "        (inputs_u_w, inputs_u_s),_ = unlabeled_iter.next()\n",
    "    except:\n",
    "        unlabeled_iter = iter(unlabeled_trainloader)\n",
    "        (inputs_u_w, inputs_u_s),_ = unlabeled_iter.next()\n",
    "\n",
    "    batch_size = inputs_x.shape[0]\n",
    "    inputs_u_w = torch.FloatTensor(inputs_u_w)\n",
    "    inputs_u_s = torch.FloatTensor(inputs_u_s)\n",
    "    inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*mu+1).to(device)\n",
    "    # test = torch.cat((inputs_x, inputs_u_w, inputs_u_s))\n",
    "    # plt.figure(figsize = (20,10))\n",
    "    # for i in np.linspace(0,120,11, dtype=int).tolist():    \n",
    "    #     img = np.moveaxis(test[i].numpy(),0,2)\n",
    "    #     plt.subplot(3,5,(i//12)+1)\n",
    "    #     plt.imshow(img)\n",
    "    targets_x = targets_x.to(device)\n",
    "    logits = net(inputs)\n",
    "    logits = de_interleave(logits, 2*mu+1)\n",
    "    logits_x = logits[:batch_size]\n",
    "    logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "    del logits\n",
    "\n",
    "    Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "    pseudo_label = torch.softmax(logits_u_w.detach(), dim=-1)\n",
    "    max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "    mask = max_probs.ge(threshold).float()\n",
    "    Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "    #Compute total loss\n",
    "    loss = Lx + lambda_u * Lu\n",
    "    #calc acc\n",
    "    acc = calculate_accuracy(logits_x, targets_x)\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    #Backpropagate Gradents\n",
    "    loss.backward()\n",
    "    #Do a single optimization step\n",
    "    optimizer.step()\n",
    "    #log the loss and acc for plotting\n",
    "    wandb.log({\"Training\": {\"Loss\" : loss.item(), \"Acc\" : acc.item()}}, step=epoch)\n",
    "    wandb.log({\"Training\": {\"Lx\" : Lx.item(), \"Lu\" : Lu.item()*2}}, step=epoch)\n",
    "    wandb.log({\"Pseudo label count\": torch.sum(mask).cpu().numpy()}, step=epoch)\n",
    "\n",
    "    return Lx.item(), Lu.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, device, loader, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #Set network in evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        #Forward pass\n",
    "        fx = net(x)\n",
    "        #Calculate loss\n",
    "        loss = loss_fn(fx, y.type(torch.LongTensor).to(device))\n",
    "        #calculate the accuracy\n",
    "        acc = calculate_accuracy(fx, y)\n",
    "        #log the cumulative sum of the loss and acc\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        #Validation set too large, stop at 20 batches\n",
    "        if i==40:\n",
    "            break\n",
    "\n",
    "    return epoch_loss/i, epoch_acc/i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "n_epochs = 2500\n",
    "lr = 5e-4\n",
    "\n",
    "net = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "net_fc_in = net.fc.in_features\n",
    "net.fc = nn.Linear(net_fc_in, out_classes).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=5e-3)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=1, epochs=n_epochs, div_factor=2, pct_start=0.25, three_phase=True, final_div_factor=1e8)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "with wandb.init(project=\"Semi-supervised Learning\", group='fixmatch', job_type='train', reinit=True, mode='online'):\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "        Lx, Lu = train(net=net, device=device, labeled_trainloader=train_loader, unlabeled_trainloader=unlabelled_loader, test_loader=test_loader, optimizer=optimizer, mu=mu, threshold=threshold, lambda_u=lambda_u, epoch=epoch)\n",
    "        \n",
    "        scheduler.step()\n",
    "        if epoch%50==0:\n",
    "            valid_loss, valid_acc = evaluate(net, device, valid_loader, loss_fn)\n",
    "            wandb.log({\"Validation\": {\"Loss\" : valid_loss, \"Acc\" : valid_acc}}, step=epoch)\n",
    "        wandb.log({\"Learning Rate\" : scheduler.get_last_lr()[0]}, step=epoch)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2acc8004557af241eccf1f459f07da2ea1a4dbde4f905bf047117aaeab484e13"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
