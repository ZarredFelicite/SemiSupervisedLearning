{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> ECE4179 - Semi-Supervised Learning Project</h1>\n",
    "<h2>FixMatch</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import STL10 as STL10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from randaugment import RandAugmentMC\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# For rotation\n",
    "from PIL import Image\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "\n",
    "####### CHANGE TO APPROPRIATE DIRECTORY TO STORE DATASET\n",
    "dataset_dir = os.getcwd()+\"/CNN-VAE/data\"\n",
    "#For MonARCH\n",
    "# dataset_dir = \"/mnt/lustre/projects/ds19/SHARED\"\n",
    "\n",
    "#All images are 3x96x96\n",
    "image_size = 96\n",
    "#Example batch size\n",
    "batch_size = 24\n",
    "# Number of classes for classification\n",
    "out_classes = 10\n",
    "start_epoch = 0\n",
    "n_epochs = 5000\n",
    "lr = 2e-4\n",
    "#No schedular used\n",
    "\n",
    "#temp = 0.5 Pseudo label temperature\n",
    "mu = 10 #Coefficient of unlabelled batch size\n",
    "threshold = 0.95 #Pseudo label threshold\n",
    "lambda_u = 2  #Coefficient of unlabelled loss\n",
    "\n",
    "start_from_checkpoint = False\n",
    "save_dir = 'Models'\n",
    "model_name = 'FixMatch'\n",
    "    \n",
    "# Hardware acceleration\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the appropriate transforms</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform random crops and mirroring for data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(image_size, padding=12, padding_mode='reflect'),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomAffine(10, translate=(0.1,0.1)),\n",
    "    #RandAugmentMC(n=1, m=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "#No random \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "stl10_mean = (0.485, 0.456, 0.406)\n",
    "stl10_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "class TransformFix(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.weak = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=96,\n",
    "                                  padding=int(96*0.125),\n",
    "                                  padding_mode='reflect')])\n",
    "        self.strong = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=96,\n",
    "                                  padding=int(96*0.125),\n",
    "                                  padding_mode='reflect'),\n",
    "            RandAugmentMC(n=2, m=10)])\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        weak = self.weak(x)\n",
    "        strong = self.strong(x)\n",
    "        return self.normalize(weak), self.normalize(strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create training and validation split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Load train and validation sets\n",
    "trainval_set = STL10(dataset_dir, split='train', transform=transform_train, download=True)\n",
    "\n",
    "#Use 10% of data for training - simulating low data scenario\n",
    "num_train = int(len(trainval_set)*0.1)\n",
    "\n",
    "#Split data into train/val sets\n",
    "torch.manual_seed(0) #Set torch's random seed so that random split of data is reproducible\n",
    "train_set, val_set = random_split(trainval_set, [num_train, len(trainval_set)-num_train])\n",
    "\n",
    "#Load test set\n",
    "test_set = STL10(dataset_dir, split='test', transform=transform_test, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get the unlabelled data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "unlabelled_set = STL10(dataset_dir, split='unlabeled', transform=TransformFix(mean=stl10_mean, std=stl10_std), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may find later that you want to make changes to how the unlabelled data is loaded. This might require you sub-classing the STL10 class used above or to create your own dataloader similar to the Pytorch one.\n",
    "https://pytorch.org/docs/stable/_modules/torchvision/datasets/stl10.html#STL10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Create the four dataloaders</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "unlabelled_loader = DataLoader(unlabelled_set, shuffle=True, batch_size=batch_size*mu, drop_last=True)\n",
    "\n",
    "valid_loader = DataLoader(val_set, batch_size=batch_size, drop_last=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "Let's use a ResNet18 architecture for our CNN..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "net_fc_in = net.fc.in_features\n",
    "net.fc = nn.Linear(net_fc_in, out_classes).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_iter = iter(train_loader)\n",
    "unlabeled_iter = iter(unlabelled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This Function will allow us to scale an images pixel values to a value between 0 and 1\n",
    "def normalize_img(img):\n",
    "    mins = img.min(0, keepdims = True).min(1, keepdims = True)\n",
    "    maxs = img.max(0, keepdims = True).max(1, keepdims = True)\n",
    "    return (img - mins)/(maxs - mins)\n",
    "\n",
    "#Visualise strongly augmented unlabeled images\n",
    "# plt.figure(figsize = (15,10))\n",
    "# (image_batch_w, image_batch_s),_ = unlabeled_iter.next()\n",
    "# for tmpC1 in range(8):    \n",
    "#     img = np.moveaxis(image_batch_s[tmpC1].numpy(),0,2)\n",
    "#     plt.subplot(2,4,tmpC1+1)\n",
    "#     plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n",
    "\n",
    "def de_interleave(x, size):\n",
    "    s = list(x.shape)\n",
    "    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning Checkpoint exists\n"
     ]
    }
   ],
   "source": [
    "#Create Save Path from save_dir and model_name, we will save and load our checkpoint here\n",
    "Save_Path = os.path.join(save_dir, model_name + \".pt\")\n",
    "\n",
    "#Create the save directory if it does not exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "#Load Checkpoint\n",
    "if start_from_checkpoint:\n",
    "    #Check if checkpoint exists\n",
    "    if os.path.isfile(Save_Path):\n",
    "        #load Checkpoint\n",
    "        check_point = torch.load(Save_Path)\n",
    "        #Checkpoint is saved as a python dictionary\n",
    "        #https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        #here we unpack the dictionary to get our previous training states\n",
    "        net.load_state_dict(check_point['model_state_dict'])\n",
    "        optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "        start_epoch = check_point['epoch']\n",
    "        best_valid_acc = check_point['valid_acc']\n",
    "        print(\"Checkpoint loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        #Raise Error if it does not exist\n",
    "        raise ValueError(\"Checkpoint Does not exist\")\n",
    "else:\n",
    "    #If checkpoint does exist and Start_From_Checkpoint = False\n",
    "    #Raise an error to prevent accidental overwriting\n",
    "    if os.path.isfile(Save_Path):\n",
    "        print(\"Warning Checkpoint exists\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device, labeled_trainloader, unlabeled_trainloader, test_loader, optimizer, loss_logger, acc_logger, mu, threshold, lambda_u):\n",
    "    #Train mode\n",
    "    net.train()\n",
    "    try:\n",
    "        inputs_x, targets_x = labeled_iter.next()\n",
    "    except:\n",
    "        labeled_iter = iter(labeled_trainloader)\n",
    "        inputs_x, targets_x = labeled_iter.next()\n",
    "    try:\n",
    "        (inputs_u_w, inputs_u_s),_ = unlabeled_iter.next()\n",
    "    except:\n",
    "        unlabeled_iter = iter(unlabeled_trainloader)\n",
    "        (inputs_u_w, inputs_u_s),_ = unlabeled_iter.next()\n",
    "\n",
    "    batch_size = inputs_x.shape[0]\n",
    "    inputs_u_w = torch.FloatTensor(inputs_u_w)\n",
    "    inputs_u_s = torch.FloatTensor(inputs_u_s)\n",
    "    inputs = interleave(torch.cat((inputs_x, inputs_u_w, inputs_u_s)), 2*mu+1).to(device)\n",
    "    # test = torch.cat((inputs_x, inputs_u_w, inputs_u_s))\n",
    "    # plt.figure(figsize = (20,10))\n",
    "    # for i in np.linspace(0,120,11, dtype=int).tolist():    \n",
    "    #     img = np.moveaxis(test[i].numpy(),0,2)\n",
    "    #     plt.subplot(3,5,(i//12)+1)\n",
    "    #     plt.imshow(img)\n",
    "    targets_x = targets_x.to(device)\n",
    "    logits = net(inputs)\n",
    "    logits = de_interleave(logits, 2*mu+1)\n",
    "    logits_x = logits[:batch_size]\n",
    "    logits_u_w, logits_u_s = logits[batch_size:].chunk(2)\n",
    "    del logits\n",
    "\n",
    "    Lx = F.cross_entropy(logits_x, targets_x, reduction='mean')\n",
    "\n",
    "    pseudo_label = torch.softmax(logits_u_w.detach_(), dim=-1)\n",
    "    max_probs, targets_u = torch.max(pseudo_label, dim=-1)\n",
    "    mask = max_probs.ge(threshold).float()\n",
    "    Lu = (F.cross_entropy(logits_u_s, targets_u, reduction='none') * mask).mean()\n",
    "    #Compute total loss\n",
    "    loss = Lx + lambda_u * Lu\n",
    "    #calc acc\n",
    "    acc = calculate_accuracy(logits_x, targets_x)\n",
    "    #zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    #Backpropagate Gradents\n",
    "    loss.backward()\n",
    "    #Do a single optimization step\n",
    "    optimizer.step()\n",
    "    #log the loss and acc for plotting\n",
    "    loss_logger.append(loss.item())\n",
    "    acc_logger.append(acc.item())\n",
    "    pseudo_label_cnt = torch.sum(mask)\n",
    "    clear_output(True)       \n",
    "    return Lx.item(), Lu.item(), loss_logger, pseudo_label_cnt, acc_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, device, loader, Loss_fun, loss_logger = None):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    #Set network in evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            #Forward pass\n",
    "            fx = net(x)\n",
    "            #Calculate loss\n",
    "            loss = Loss_fun(fx, y.type(torch.LongTensor).to(device))\n",
    "            #calculate the accuracy\n",
    "            acc = calculate_accuracy(fx, y)\n",
    "            #log the cumulative sum of the loss and acc\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            #log the loss for plotting if we passed a logger to the function\n",
    "            if not (loss_logger is None):\n",
    "                loss_logger.append(loss.item())\n",
    "            print(\"EVALUATION: | Itteration [%d/%d] | Loss %.2f | Accuracy %.2f |\" %(i+1 ,len(loader), loss.item(), 100*(epoch_acc/(i+1))))\n",
    "            clear_output(True)\n",
    "            if i==20:\n",
    "                break\n",
    "    #return the avaerage loss and acc from the epoch as well as the logger array       \n",
    "    return epoch_loss/i, epoch_acc/i, loss_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-be1475dd1571>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mLx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpseudo_label_cnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_acc_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabeled_trainloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munlabeled_trainloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0munlabelled_loader\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mtest_loader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_logger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_loss_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_logger\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_acc_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_u\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlambda_u\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_loss_logger\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-3c9ddd2797ce>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(net, device, labeled_trainloader, unlabeled_trainloader, test_loader, optimizer, loss_logger, acc_logger, mu, threshold, lambda_u)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m#log the loss and acc for plotting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mloss_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[0macc_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mpseudo_label_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cycle_loss = 0\n",
    "cycle_loss_av = 0\n",
    "pseudo_label_cnt = 0\n",
    "pseudo_label_cnt_log = []\n",
    "test_accs = []\n",
    "training_loss_logger = []\n",
    "Lx_logger = []\n",
    "Lu_logger = []\n",
    "training_acc_logger = []\n",
    "train_loss_average = []\n",
    "cycle_loss_log = []\n",
    "validation_acc_logger = []\n",
    "validation_loss_logger = []\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "net.zero_grad()\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs):\n",
    "    Lx, Lu, training_loss_logger, pseudo_label_cnt, training_acc_logger = train(net=net, device=device, labeled_trainloader=train_loader, unlabeled_trainloader=unlabelled_loader,   test_loader=test_loader, optimizer=optimizer, loss_logger=training_loss_logger, acc_logger=training_acc_logger, mu=mu, threshold=threshold, lambda_u=lambda_u)\n",
    "    if epoch%20==0:\n",
    "        valid_loss, valid_acc, validation_loss_logger = evaluate(net, device, valid_loader, loss_fn, validation_loss_logger)\n",
    "    validation_acc_logger.append(valid_acc)\n",
    "    pseudo_label_cnt = pseudo_label_cnt.cpu().numpy()\n",
    "    pseudo_label_cnt_log.append(pseudo_label_cnt)\n",
    "    Lx_logger.append(Lx)\n",
    "    Lu_logger.append(Lu*2)\n",
    "    if epoch%20==0:\n",
    "        plt.figure(figsize = (15,10))\n",
    "        plt.subplot(2,3,1)\n",
    "        plt.title('Train Loss Total')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(training_loss_logger, c = \"b\")\n",
    "        plt.subplot(2,3,2)\n",
    "        plt.title('Train Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.plot(training_acc_logger, c = \"b\")\n",
    "        plt.subplot(2,3,3)\n",
    "        plt.title('Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.plot(validation_acc_logger, c = \"b\")\n",
    "        plt.subplot(2,3,4)\n",
    "        plt.title('Contributing Pseudo Labels')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Count per batch')\n",
    "        plt.plot(pseudo_label_cnt_log, c = \"b\")\n",
    "        plt.subplot(2,3,5)\n",
    "        plt.title('Loss: Labeled')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(Lx_logger, c = \"b\")\n",
    "        plt.subplot(2,3,6)\n",
    "        plt.title('Loss: Unlabeled')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.plot(Lu_logger, c = \"b\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.8.3 64-bit (conda)",
   "display_name": "Python 3.8.3 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "47aed16c4bc5e22eb29adbd612f5eee02d058a02095cad432c53cdb0bc166e12"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}